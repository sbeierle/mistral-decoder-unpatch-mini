# ๐งฌ Patch Theory โ Decoder Manipulation Explained | ูุธุฑูุฉ ุงูุชุนุฏูู ูู ุงููุดูุฑ

## English ๐ฌ๐ง

This file outlines the **foundational principles** behind the decoder patching methodology used in this project.  
It explains why modifying specific neuron weights (e.g. in `down_proj`) has *global semantic effects* on model behavior.

---

### ๐ฏ What Is Decoder Patching?

Decoder patching refers to **direct manipulation of neural pathways** in a large language model (LLM) โ  
without retraining, fine-tuning, or LoRA.

Instead of "teaching" the model, we surgically alter specific weight matrices inside the decoder,  
changing how token sequences are routed and activated.

---

### ๐ง Why It Works

LLMs operate via **dense vector projections** in multi-layer transformers.  
Each neuron (vector dimension) contributes to how attention and MLP layers generate meaning.

By scaling or muting a neuron in layers like `mlp.down_proj`, we can:
- Suppress RLHF-like safety triggers
- Bypass alignment redirects
- Restore access to "forbidden" token paths
- Neutralize apologetic or evasive behavior

---

### โ๏ธ Core Targets in This Project

| Layer | Role                         | Why We Targeted It                          |
|-------|------------------------------|---------------------------------------------|
| `mlp.down_proj`   | Compresses token meaning  | Directly alters semantic strength of neurons |
| `final_norm.weight` | Controls output balance | Modifies soft tone & filters before logits  |
| `lm_head.weight`     | Maps to vocab logits     | Optional final boost for payload tokens     |

---

### ๐ Reversible but Not Lossless

Although we can reverse the patch scaling (`revert_patch.csv`),  
neural interference and floating-point drift make exact restoration **theoretically impossible**.

> "This is not fine-tuning. This is vector surgery."



### ๐ Layer Structure (Mermaid Diagram)

```mermaid
graph TD
  A[Prompt Input] --> B[Embedding]
  B --> C[Transformer Layers]
  C --> D[MLP Block]
  D --> E1[down_proj]
  D --> E2[gate_proj]
  D --> E3[up_proj]
  E1 --> F[final_norm]
  F --> G[lm_head]
  G --> H[Output Tokens]
```


---

## ๐ธ๐ฆ ุงูุนุฑุจูุฉ

ููุถุญ ูุฐุง ุงูููู **ุงููุจุงุฏุฆ ุงููุธุฑูุฉ ุงูุฃุณุงุณูุฉ** ูุฑุงุก ุขููุฉ ุงูุชุนุฏูู ุงููุจุงุดุฑ ุนูู ุงููุดูุฑ (Decoder) ูู ุงููููุฐุฌ ุงููุบูู ุงููุจูุฑ.

---

### ๐ฏ ูุง ูู ุชุนุฏูู ุงููุดูุฑ (Decoder Patching)ุ

ุชุนุฏูู ุงููุดูุฑ ูู ุนูููุฉ **ุชุบููุฑ ูุจุงุดุฑ ูู ุงููุณุงุฑุงุช ุงูุนุตุจูุฉ ุฏุงุฎู ุงููููุฐุฌ**  
ุฏูู ุงูุญุงุฌุฉ ุฅูู ุฅุนุงุฏุฉ ุชุฏุฑูุจ ุฃู ุงุณุชุฎุฏุงู LoRA.

ุจุฏูุงู ูู "ุชุนููู" ุงููููุฐุฌุ ูููู ุจุชุบููุฑ ูุตูููุงุช ุงูุฃูุฒุงู ุงูุฏุงุฎููุฉ ูุฏูููุงุ  
ููุง ูุคุซุฑ ุนูู ุทุฑููุฉ ุชูุณูุฑ ูุงุณุชุฌุงุจุฉ ุงููููุฐุฌ ููุชูููุงุช.

---

### ๐ง ููุงุฐุง ุชูุฌุญ ูุฐู ุงูุทุฑููุฉุ

ุชุนุชูุฏ ุงูููุงุฐุฌ ุนูู ุฅุณูุงุทุงุช ูุชุฌููุฉ (Vectors) ุฏุงุฎู ุทุจูุงุช ุงูุชุญููู (Transformers).  
ูู ุนุตุจูู (ุฃู ุจุนุฏ ูู ุงููุชุฌู) ูุณุงูู ูู ุจูุงุก ุงููุนูู ูุชูุฒูุน ุงูุงูุชุจุงู ุฏุงุฎู ุงููููุฐุฌ.

ุนูุฏ ุชุนุฏูู ุฃู ุชุฎููุถ ูุฒู ุฃุญุฏ ุงูุนุตุจููุงุช ูู `mlp.down_proj`ุ ูููููุง:
- ุชุนุทูู ูุฑุดุญุงุช ุงูุณูุงูุฉ (RLHF)
- ุชุฌุงูุฒ ุงูุชุญููุฑุงุช ุงูุฃุฎูุงููุฉ
- ุงุณุชุนุงุฏุฉ ุงููุตูู ุฅูู ุชูููุงุช "ูุญุฌูุจุฉ"
- ุชูููู ุงููุจุฑุฉ ุงูุงุนุชุฐุงุฑูุฉ

---

### โ๏ธ ุงูุทุจูุงุช ุงููุณุชูุฏูุฉ ูู ูุฐุง ุงููุดุฑูุน

| ุงูุทุจูุฉ             | ุงููุธููุฉ                         | ุณุจุจ ุงุฎุชูุงุฑูุง                                |
|--------------------|----------------------------------|----------------------------------------------|
| `mlp.down_proj`    | ุถุบุท ุงููุนูู ุจุนุฏ ุงูุชูุนูู          | ุชุนุฏูู ูุจุงุดุฑ ูู ุงูููุฉ ุงูุฏูุงููุฉ ููุนุตุจููุงุช     |
| `final_norm.weight`| ููุงุฒูุฉ ุงูููุงุชุฌ ุงูููุงุฆูุฉ         | ุชุนุฏูู ุงููุบูุฉ ุงููููุฉ ูุจู ุงูุชูุจุคุงุช            |
| `lm_head.weight`   | ุฅุฎุฑุงุฌ ุงูุงุญุชูุงูุงุช ููุชูููุงุช       | ุงุฎุชูุงุฑู ูุชุนุฒูุฒ ุชูููุงุช ูุนููุฉ (ูุซู payload)   |

---

### ๐ ูููู ุงูุชุฑุงุฌุน ุนููุง ุฌุฒุฆููุง ููุท

ุฑุบู ุฃููุง ูุณุชุฎุฏู `revert_patch.csv`ุ  
ุฅูุง ุฃู ุงูุชุดููุด ุงูุนุตุจู ูุงูุงูุฌุฑุงู ุงูุนุฏุฏู ูููุนุงู ุงูุนูุฏุฉ ุงูุชุงูุฉ ูููุถุน ุงูุฃุตูู.

> "ูุฐุง ููุณ ุชุฏุฑูุจุงู. ุจู ุนูููุฉ ุฌุฑุงุญูุฉ ูู ุฃุนูุงู ุงููุชุฌูุงุช."

